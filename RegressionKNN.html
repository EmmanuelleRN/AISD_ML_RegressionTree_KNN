<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>~</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my_css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# ~
]

---


class: class: center, middle


&lt;!-- css: [default, duke-blue, hygge-duke] --&gt;

<div>
<style type="text/css">.xaringan-extra-logo {
width: 110px;
height: 128px;
z-index: 0;
background-image: url(www/channels4_profile-removebg-preview.png);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:1em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.title-slide):not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>



&lt;style type="text/css"&gt;
.regression table {
  font-size: 12px;     
}

.dataTables_info{
  font-size: 10px;
}

.dataTables_paginate{
  font-size: 10px;
}

.dataTables_length{
  font-size: 10px;
}
&lt;/style&gt;


# Machine Learning

---

# Machine Learning - Concepts

- Machine Learning (ML) is a subset of Artificial Intelligence (AI)
- Algorithms that can improve automatically through experience and by the use of data without being explicit programmed, reason why we say that the algorithms learn.
- With ML algorithms we can build a model to make predictions or decisions.
- Machine learning algorithms are used many different applications, for example:
  - Medicine
  - Email filtering
  - Speech recognition
  - Computer vision

&lt;center&gt;&lt;img src="https://backend.mile.cloud/upload/module/3ab28920802a8e64b800fdd2d22e7940.png" height="200"&gt;&lt;/img&gt;&lt;/center&gt;

---

class: center, middle
background-image: url(www/machine-learning.png)
background-size: contain

---

class: center, middle

# Supervised Learning

---

# Supervised Learning

.pull-left[
**Supervised learning** is where you have input variables ($X$) and an output variable ($y$) and you use an algorithm to learn the mapping function from the input to the output.

&lt;center&gt; y = f(X) &lt;/center&gt;

- It is the most common type of Machine Learning problem

- It is called **supervised** because we have the label that tell us the correct information, and we are going to be corrected if we predict wrong.

- Supervised learning can be grouped into two problems:
  - **Regression:** The output variable is a real number, for example, weight
  - **Classification:** The output variable is a category, for example, disease and no disease 
]

.pull-right[
&lt;img src="https://maplearn.readthedocs.io/en/latest/_images/classif_reg.png"&gt;&lt;/img&gt;
]

---

class: center, middle

# Classification

---

# Classification

**Classification** is a type of **supervised** learning where we categorise data into classes. There are many different algorithms that can help us solve this kind of problems.

Classification requires a training dataset with many examples of inputs and outputs from which to learn. It can be categorised in two types of problems:
  - **Binary classification:** The outcome has only **two** labels, for example, disease and not disease. 
    - Some popular algorithms are: Logistic Regression, Decision Tree, K-Nearest Neighbour (KNN)
  - **Multi-label classification:** The outcome has multiple labels, for example, dog, cat, bird and other.
    - Some popular algorithms: KNN, Decision Tree, Random Forest, Naive Bayes

To evaluate the model performance we can make use of ROC, confusion matrix, etc. We need to be aware of **class imbalance** problems.

---

class: center, middle

# Decision Tree (CART)

---

# Decision Tree

There are various algorithms that can grow a tree. 
- Differences: 
  - Possible structure of the tree (e.g. number of splits per node)
  - Criteria how to find the splits
  - Criteria to stop splitting 
  - How to estimate the simple models within the leaf nodes. 
  
The **Classification and Regression Trees (CART)** algorithm is probably the most popular algorithm for tree induction. 
  - We will focus on CART, but the interpretation is similar for most other tree types.
  
&gt; Note: Decision Trees can be used for both Regression and Classification problems

---

# Theory

The processes behind *classification* and *regression* in tree analysis is very similar, but we need to first distinguish the two.

- **Classification:** For a response variable which has *classes*, we want to organize the dataset into groups by the response variable. 
- **Regression:** When our response variable is instead numeric or continuous we
wish to use the data to predict the outcome, and will use regression trees in this situation. 

Essentially, a classification tree splits the data based on homogeneity by categorizing the data based on similarity, filtering out the "noise" and making the data "pure", hence the concept of **purity criterion**. 

When the response variable does not have classes, a regression model is fit to each of the independent variables, isolating these variables as nodes where their inclusion decreases error. 

&lt;!-- --- --&gt;

&lt;!-- # Theory - Regression --&gt;

&lt;!-- .pull-left[ --&gt;
&lt;!-- In traditional regression models, we have a single model to represent the entire data set. CART is an alternative approach to this, where the data is --&gt;
&lt;!-- partitioned into smaller sections where variable interactions are more clear. --&gt;

&lt;!-- CART uses this recursive partitioning to create a tree where each node represents a cell of the partition. Each cell has attached to it a simplified model which is applied to that cell only, and as we move down the nodes, or leaves, of the tree we are conditioning on a certain variable.  --&gt;

&lt;!-- In the figure, A, B, and C are each terminal nodes, implying that after this split, further splitting does not explain enough of the variance to be relevant in describing Y. --&gt;
&lt;!-- ] --&gt;

&lt;!-- .pull-right[ --&gt;
&lt;!-- &lt;img src="www/tree.PNG" height="200"&gt;&lt;/img&gt; --&gt;
&lt;!-- ] --&gt;

&lt;!-- --- --&gt;

---

# Theory

CART takes a feature and determines which cut-off point minimizes:
- The variance of `\(Y\)` for a regression task 
  -  The variance tells us how much the y values in a node are spread around their mean value
- The Gini index of the class distribution of `\(Y\)` for classification tasks
  - The Gini index tells us how "impure" a node is, e.g. if all classes have the same frequency, the node is impure, if only one class is present, it is maximally pure. 

Variance and Gini index are minimized when the data points in the nodes have very similar values for `\(Y\)`. As a consequence, the best cut-off point makes the two resulting subsets as different as possible with respect to the target outcome. 

For categorical features, the algorithm tries to create subsets by trying different groupings of categories. After the best cutoff per feature has been determined, the algorithm selects the feature for splitting that would result in the best partition in terms of the variance or Gini index and adds this split to the tree. The algorithm continues this search-and-split recursively in both new nodes until a stop criterion is reached. Possible criteria are: A minimum number of instances that have to be in a node before the split, or the minimum number of instances that have to be in a terminal node.

---

# Interpretation

.pull-left[

Let's first define some keys terms:
- **Root node:** The base of the decision tree.
- **Splitting:** The process of dividing a node into multiple sub-nodes.
- **Decision node:** When a sub-node is further split into additional sub-nodes.
- **Leaf node:** When a sub-node does not further split into additional sub-nodes; represents possible outcomes.
- **Pruning:** The process of removing sub-nodes of a decision tree.
- **Branch:** A subsection of the decision tree consisting of multiple nodes.

Starting from the root node, you go to the next nodes and the edges tell you which subsets you are looking at. Once you reach the leaf node, the node tells you the predicted outcome. All the edges are connected by 'AND', so we are conditioning on the variable.

]

.pull-right[

![tree](www/tree-graphic.webp)
For example, if you are with friends *AND* it is windy, *THEN* it is above par.

]
---

# In R

There are two main libraries that implement the CART model:
- tree
  - Command: *tree(formula, data, weights, subset, na.action = na.pass, control = tree.control(nobs, ...), method = "recursive.partition", split = c("deviance", "gini"), model = FALSE, x = FALSE, y = TRUE, wts = TRUE, ...)*
      - **formula** defines if it is classification or regression. It should be entered on the format `\(Y \sim x_1 + ... + x_n\)`
      - **data**: Dataset 
- rpart
  - Command: *rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)*
      - **method** defines if it is a classification or regression

In summary, if using `\(tree\)` for:
 - **Classification**: `\(\text{tree}(\text{factor}(Y) \sim \sum x, \text{data})\)`
 - **Regression**: `\(\text{tree}(Y \sim \sum x, \text{data})\)`

If using `\(rpart\)`:
- **Classification**: `\(\text{tree}(Y \sim \sum x, \text{data}, method = 'class')\)`
- **Regression**: `\(\text{tree}(Y \sim \sum x, \text{data}, method = 'anova')\)`

---

# Example

We will use a **classification** dataset for the example. The dataset will be the `Pima Indians Diabetes Database` from `mlbench`.


```r
library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(mlbench)     # ML datasets

# Pima Indians Diabetes Database
data(PimaIndiansDiabetes)

glimpse(PimaIndiansDiabetes)
```

```
## Rows: 768
## Columns: 9
## $ pregnant &lt;dbl&gt; 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1…
## $ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…
## $ pressure &lt;dbl&gt; 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60, 72, 0,…
## $ triceps  &lt;dbl&gt; 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, 0, 47, 0…
## $ insulin  &lt;dbl&gt; 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175, 0, 230…
## $ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37…
## $ pedigree &lt;dbl&gt; 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158…
## $ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…
## $ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…
```


---

# Example

First, we will split the data into a training and test set


```r
# Create training (70%) and test (30%) sets 
# Use set.seed for reproducibility
set.seed(123)
pima_split &lt;- initial_split(PimaIndiansDiabetes, prop = .7)
pima_train &lt;- training(pima_split)
pima_test  &lt;- testing(pima_split)
```

---

# Example - Implementation

I'll fit the example using `rpart` and plot the tree using `rpart.plot`


```r
tree &lt;- rpart(formula = diabetes ~ .,
              data = pima_train,
              method = 'class')

tree
```

```
## n= 537 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##    1) root 537 187 neg (0.65176909 0.34823091)  
##      2) glucose&lt; 143.5 414  96 neg (0.76811594 0.23188406)  
##        4) age&lt; 28.5 226  28 neg (0.87610619 0.12389381)  
##          8) mass&lt; 29.95 107   2 neg (0.98130841 0.01869159) *
##          9) mass&gt;=29.95 119  26 neg (0.78151261 0.21848739)  
##           18) pressure&gt;=51 107  18 neg (0.83177570 0.16822430) *
##           19) pressure&lt; 51 12   4 pos (0.33333333 0.66666667) *
##        5) age&gt;=28.5 188  68 neg (0.63829787 0.36170213)  
##         10) mass&lt; 27.05 43   4 neg (0.90697674 0.09302326) *
##         11) mass&gt;=27.05 145  64 neg (0.55862069 0.44137931)  
##           22) glucose&lt; 108.5 58  15 neg (0.74137931 0.25862069) *
##           23) glucose&gt;=108.5 87  38 pos (0.43678161 0.56321839)  
##             46) pedigree&lt; 0.7205 67  32 neg (0.52238806 0.47761194)  
##               92) age&lt; 30.5 16   4 neg (0.75000000 0.25000000) *
##               93) age&gt;=30.5 51  23 pos (0.45098039 0.54901961)  
##                186) triceps&lt; 36.5 44  21 neg (0.52272727 0.47727273)  
##                  372) age&gt;=44.5 14   4 neg (0.71428571 0.28571429) *
##                  373) age&lt; 44.5 30  13 pos (0.43333333 0.56666667)  
##                    746) age&lt; 41.5 22   9 neg (0.59090909 0.40909091)  
##                     1492) pedigree&gt;=0.256 13   3 neg (0.76923077 0.23076923) *
##                     1493) pedigree&lt; 0.256 9   3 pos (0.33333333 0.66666667) *
##                    747) age&gt;=41.5 8   0 pos (0.00000000 1.00000000) *
##                187) triceps&gt;=36.5 7   0 pos (0.00000000 1.00000000) *
##             47) pedigree&gt;=0.7205 20   3 pos (0.15000000 0.85000000) *
##      3) glucose&gt;=143.5 123  32 pos (0.26016260 0.73983740)  
##        6) mass&lt; 29.85 22   9 neg (0.59090909 0.40909091) *
##        7) mass&gt;=29.85 101  19 pos (0.18811881 0.81188119) *
```

---

# Explaining the output

The output explains the steps for the split. For example, 
- We start with 537 observations at the root node (very beginning) 
- The first variable we split on (the first variable that optimizes a reduction in the Gini index) is `glucose`. 
- We can see that at the first node all observations with `glucose &lt; 143.5` go to the 2nd (`2)`) branch. 
- The total number of observations that follow this branch (`414`), their probability of not having diabetes is `0.768` and we have `96` negatives. 
  - If you look for the 3rd branch (`3)`) you will see that `123` observations with `glucose &gt;= 143.5` follow this branch and their probability of not having diabetes is 0.26. 
- Basically, this is telling us the most important variable that has the largest reduction in Gini initially is `glucose` with the majority of people having lower glucose values resulting in not having diabetes.

---

# Example - Visualise Tree

We can visualize our model with `rpart.plot` that has many plotting options. One thing you may notice is that this tree contains 13 internal nodes resulting in 14 terminal nodes. 


```r
rpart.plot(tree)
```

![](RegressionKNN_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

---

# Explaining the Partitions

You might have noticed that not all the variables were used to generate the plot. 

Behind the scenes `rpart` is automatically applying a range of cost complexity to prune the tree. To compare the error for each value, rpart performs a 10-fold cross validation so that the error associated with a given value is computed on the hold-out validation data. 

In this example we find diminishing returns after 14 terminal nodes as illustrated below. We could use a tree with 3 terminal nodes and reasonably expect to experience similar results within a small margin of error.


```r
plotcp(tree)
```

![](RegressionKNN_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;

---

# Pruning

By default, `rpart` is performing some automated tuning, with an optimal subtree of 13 splits, 14 terminal nodes, and a cross-validated error of 0.7433. However, we can perform additional tuning to try improve model performance.


```r
tree$cptable
```

```
##           CP nsplit rel error    xerror       xstd
## 1 0.31550802      0 1.0000000 1.0000000 0.05903724
## 2 0.02139037      1 0.6844920 0.6844920 0.05280048
## 3 0.01960784      2 0.6631016 0.7326203 0.05402086
## 4 0.01782531      7 0.5614973 0.7326203 0.05402086
## 5 0.01604278     10 0.5080214 0.7058824 0.05335630
## 6 0.01069519     11 0.4919786 0.7165775 0.05362609
## 7 0.01000000     13 0.4705882 0.7433155 0.05427754
```

---

# Tuning

In addition to the cost complexity ($\alpha$ parameter), it is also common to tune:

- **minsplit:** the minimum number of data points required to attempt a split before it is forced to create a terminal node. 
- **maxdepth:** the maximum number of internal nodes between the root node and the terminal nodes. 



```r
tree_control &lt;- rpart(
    formula = diabetes ~ .,
    data    = pima_train,
    method  = "class", 
    control = list(minsplit = 10, maxdepth = 12, xval = 10)
)

tree_control$cptable
```

```
##           CP nsplit rel error    xerror       xstd
## 1 0.31550802      0 1.0000000 1.0000000 0.05903724
## 2 0.02139037      1 0.6844920 0.7005348 0.05321939
## 3 0.01960784      2 0.6631016 0.7272727 0.05389058
## 4 0.01782531      7 0.5614973 0.7272727 0.05389058
## 5 0.01604278     10 0.5080214 0.7112299 0.05349187
## 6 0.01069519     11 0.4919786 0.7005348 0.05321939
## 7 0.01000000     13 0.4705882 0.7165775 0.05362609
```

---

# Tuning

This approach requires you to manually assess multiple models. 

We can perform a grid search to automatically search across a range of differently tuned models to identify the optimal hyerparameter setting.


```r
hyper_grid &lt;- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 15, 1)
)

# total number of combinations
nrow(hyper_grid)
```

```
## [1] 128
```

```r
models &lt;- list()

for (i in 1:nrow(hyper_grid)) {
  # train a model and store in the list
  models[[i]] &lt;- rpart(
    formula = diabetes ~ .,
    data    = pima_train,
    method  = "class",
    control = list(minsplit = hyper_grid$minsplit[i], maxdepth = hyper_grid$maxdepth[i])
    )
}
```

---

# Tuning

We can now create a function to extract the minimum error associated with the optimal cost complexity value for each model. 


```r
# function to get optimal cp
get_cp &lt;- function(x) {
  min    &lt;- which.min(x$cptable[, "xerror"])
  cp &lt;- x$cptable[min, "CP"] 
}

# function to get minimum error
get_min_error &lt;- function(x) {
  min    &lt;- which.min(x$cptable[, "xerror"])
  xerror &lt;- x$cptable[min, "xerror"] 
}

hyper_grid %&gt;%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)
    ) %&gt;%
  arrange(error) %&gt;%
  top_n(-5, wt = error)
```

```
##   minsplit maxdepth         cp     error
## 1        8       13 0.01069519 0.6470588
## 2       11       10 0.01604278 0.6577540
## 3       13       15 0.01069519 0.6577540
## 4        9        9 0.01069519 0.6631016
## 5        6        8 0.01604278 0.6684492
## 6       10       13 0.01000000 0.6684492
## 7       17       15 0.01604278 0.6684492
```

---

# Example - Applying best model and predict

We can apply this final optimal model and predict on our test set. 


```r
optimal_tree &lt;- rpart(
    formula = diabetes ~ .,
    data    = pima_train,
    method  = "class",
    control = list(minsplit = 8, maxdepth = 13, cp = 0.01069519)
    )

pred &lt;- predict(optimal_tree, newdata = pima_test) %&gt;% as.data.frame() %&gt;%
  mutate(class = as.factor(ifelse(neg &gt; pos, "neg", "pos")))
performanceEstimation::classificationMetrics(pima_test$diabetes, pred$class, 
                                             metrics = c("rec", "prec", "F", "acc"))
```

```
##       rec      prec         F       acc 
## 0.8733333 0.7359551 0.7987805 0.7142857
```

---

class: class: center, middle

# KNN

---

# KNN

The K-Nearest Neighbours (KNN) is a **supervised learning** algorithm that can be used for regression and classification tasks, but it is more comonly used for classification.
- K represents the number of neighbours that will be used
- It assumes that similar things will be close to each other and dissimilar things will be far from each other.
- To understand the similarity between observations, we need to know the distance between them.
  - There are different distance metrics that we can use depending on the problem we are solving.
  
When usign KNN we need to normalise our data, as variables we different scale might have a bigger impact simply because of the scale.
  
&lt;img src="www/knn.png" width="300px" /&gt;

---

# The KNN Algorithm

1. Load the data
2. Initialise K
  2.a) K is a user defined choice
3. For each observation in the data
  3.a) Calculate the distance between the current observation and all the data points in the dataset
  3.b) Sort the distances from smallest to largest
  3.c) Pick the first K entries from the sorted collection
4. If regression, return the mean of the K labels and if classification return the most common label

# Choosing the right value for K

If we pick the wrong K for the data, our prediction is going to be wrong. We need to run the algorithm for different K values to find the K that reduces the errors we encounter whilst maintaining a good performance for unseen data.

---

# Example - KNN

We will continue to use the same diabetes dataset.

We will be using the library `recipes` to create workflows to apply the normalisation step with the use of the functions `step_scale` and `step_center` and we will build the model using the `nearest_neighbor` function from parsnip with the engine `kknn`.


```r
library(recipes)
library(workflows)

diabetes_recipe &lt;- recipes::recipe(diabetes ~ ., data = PimaIndiansDiabetes) |&gt;
  recipes::step_scale(recipes::all_predictors()) |&gt;
  recipes::step_center(recipes::all_predictors())

diabetes_spec &lt;- parsnip::nearest_neighbor(neighbors = parsnip::tune()) |&gt;
  parsnip::set_engine("kknn") |&gt;
  parsnip::set_mode("classification")
```

We can notice that for the number K of neighbours we passed the function `tune`, meaning that we are going to tune the value of K.
---

# Example - Train KNN

To train the model we will use 5 fold cross validation. 


```r
diabetes_cv &lt;- rsample::vfold_cv(pima_train, v = 5)

diabetes_wkflw &lt;- workflows::workflow() |&gt;
  workflows::add_recipe(diabetes_recipe) |&gt;
  workflows::add_model(diabetes_spec)

diabetes_wkflw
```

```
## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: nearest_neighbor()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 2 Recipe Steps
## 
## • step_scale()
## • step_center()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## K-Nearest Neighbor Model Specification (classification)
## 
## Main Arguments:
##   neighbors = parsnip::tune()
## 
## Computational engine: kknn
```

---

# Example - Train KNN

Next, we run cross validation for a grid of neighbours ranging from 1 to 20.


```r
grid_vals &lt;- tibble::tibble(neighbors = seq(from = 1, to = 20, by = 1))

diabetes_results &lt;- diabetes_wkflw |&gt;
  tune::tune_grid(resamples = diabetes_cv, grid = grid_vals) 

diabetes_results |&gt;
  tune::collect_metrics() 
```

```
## # A tibble: 40 × 7
##    neighbors .metric  .estimator  mean     n std_err .config              
##        &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1         1 accuracy binary     0.698     5 0.00808 Preprocessor1_Model01
##  2         1 roc_auc  binary     0.664     5 0.00772 Preprocessor1_Model01
##  3         2 accuracy binary     0.698     5 0.00808 Preprocessor1_Model02
##  4         2 roc_auc  binary     0.724     5 0.0193  Preprocessor1_Model02
##  5         3 accuracy binary     0.698     5 0.00808 Preprocessor1_Model03
##  6         3 roc_auc  binary     0.745     5 0.0227  Preprocessor1_Model03
##  7         4 accuracy binary     0.698     5 0.00808 Preprocessor1_Model04
##  8         4 roc_auc  binary     0.758     5 0.0204  Preprocessor1_Model04
##  9         5 accuracy binary     0.721     5 0.00663 Preprocessor1_Model05
## 10         5 roc_auc  binary     0.764     5 0.0197  Preprocessor1_Model05
## # … with 30 more rows
```


---

# KNN - Run final model with tunned parameters

We can finish the workflow by using the function `finalize_workflow` alongside the function `select_best` that selects the best parameter combination from the grid search.


```r
(diabetes_wkflw &lt;- diabetes_wkflw |&gt;
  tune::finalize_workflow(diabetes_results |&gt;
                            tune::select_best(metric = "roc_auc")))
```

```
## ══ Workflow ════════════════════════════════════════════════════════════════════
## Preprocessor: Recipe
## Model: nearest_neighbor()
## 
## ── Preprocessor ────────────────────────────────────────────────────────────────
## 2 Recipe Steps
## 
## • step_scale()
## • step_center()
## 
## ── Model ───────────────────────────────────────────────────────────────────────
## K-Nearest Neighbor Model Specification (classification)
## 
## Main Arguments:
##   neighbors = 20
## 
## Computational engine: kknn
```

---

# KNN - Evaluate the model on the test set

So far we have:
  - Defined our recipe
  - Defined the model
  - Tuned the model’s parameters
  
Now, we’re ready to fit the final model. All the steps are contained within the `workflow` object, so we can apply the `last_fit()` function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set.


```r
diabetes_fit &lt;- diabetes_wkflw |&gt;
  # fit on the training set and evaluate on test set
  tune::last_fit(pima_split)

diabetes_fit
```

```
## # Resampling results
## # Manual resampling 
## # A tibble: 1 × 6
##   splits            id               .metrics .notes   .predictions .workflow 
##   &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    
## 1 &lt;split [537/231]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;
```

---

# KNN - Seeing the result of our model

To see the performance of the test set, we can use again the function `collect_metrics()`


```r
diabetes_fit |&gt;
  tune::collect_metrics()
```

```
## # A tibble: 2 × 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.779 Preprocessor1_Model1
## 2 roc_auc  binary         0.825 Preprocessor1_Model1
```


---

# References

Majid (2013)

Breiman et al. (1984)
---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
